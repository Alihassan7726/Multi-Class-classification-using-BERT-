# Multi-Class-classification-using-BERT

Bidirectional Encoder Representation from Transformers (BERT) is designed to pretrain deep bidirectional representations from
unlabeled text by jointly conditioning on both left and right context in all layers and have achieved State-Of-Art results on many NLP tasks like Question Answering , Sentiment analysis/classification , language inference and much more 

![image](https://www.google.com/url?sa=i&url=https%3A%2F%2Fmedium.com%2Fdair-ai%2Fa-light-introduction-to-bert-2da54f96b68c&psig=AOvVaw2dtVHIK2SuRCsQCNLjrxWc&ust=1624823445406000&source=images&cd=vfe&ved=0CAoQjRxqFwoTCKCGppyJtvECFQAAAAAdAAAAABAD)

